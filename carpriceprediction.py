# -*- coding: utf-8 -*-
"""carpriceprediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Ostryc3_YhtMdeUnOnfVFt9cSI2SzXr
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('C:\Users\Lenovo\Desktop\SEM6\DMA(lab)\car_data.csv')

df.head()

df = df.drop(columns=['engine','id',])

# Get unique values in 'brand' column
unique_brands = df['brand'].unique()
print("Unique brands:", unique_brands)


unique_fuels = df['fuel_type'].unique()
print("Unique fuels:", unique_fuels)

unique_transmission = df['transmission'].unique()
print("Unique transmissions:", unique_transmission)

df['clean_title'] = df['clean_title'].map({'Yes': 1, 'No': 0})

from sklearn.preprocessing import LabelEncoder
label_enc_accident = LabelEncoder()
df['accident_label_enc'] = label_enc_accident.fit_transform(df['accident'])
print("accident label encoded:", df['accident_label_enc'].tolist())

label_enc_clean_title = LabelEncoder()
df['clean_title_label_enc'] = label_enc_clean_title.fit_transform(df['clean_title'])
print("clean_title label encoded:", df['clean_title_label_enc'].tolist())

brand_ohe = pd.get_dummies(df['brand'], prefix='brand')
print("Brand One-Hot Encoding:\n", brand_ohe)

# One-hot encode 'fuel_type'
fuel_ohe = pd.get_dummies(df['fuel_type'], prefix='fuel_type')
print("Fuel Type One-Hot Encoding:\n", fuel_ohe)

# One-hot encode 'transmission'
trans_ohe = pd.get_dummies(df['transmission'], prefix='transmission')
print("Transmission One-Hot Encoding:\n", trans_ohe)

# One-hot encode 'ext_col'
ext_col_ohe = pd.get_dummies(df['ext_col'], prefix='ext_col')
print("Exterior Color One-Hot Encoding:\n", ext_col_ohe)

# One-hot encode 'int_col'
int_col_ohe = pd.get_dummies(df['int_col'], prefix='int_col')
print("Interior Color One-Hot Encoding:\n", int_col_ohe)

from datetime import datetime

current_year = datetime.now().year

# Compute car age
df['car_age'] = current_year - df['model_year']

print(df[['model_year', 'car_age']])

df_model = pd.concat([df, brand_ohe, fuel_ohe, trans_ohe, ext_col_ohe, int_col_ohe], axis=1)

df_model = df_model.drop(columns=['brand', 'fuel_type', 'transmission', 'ext_col', 'int_col','model','model_year', 'accident'])

X = df_model.drop('price', axis=1)
y = df_model['price']

# Train-test split
X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

# Fit on train data and transform both train and test
X_train[['milage']] = scaler.fit_transform(X_train[['milage']])
X_test[[ 'milage']] = scaler.transform(X_test[['milage']])

# --- Decision Tree ---
dt = DecisionTreeRegressor(max_depth=9, min_samples_split=20, random_state=42)
dt.fit(X_train, Y_train)

Y_pred_dt_train = dt.predict(X_train)
Y_pred_dt_test = dt.predict(X_test)

print(f"Decision Tree - Train MSE: {mean_squared_error(Y_train, Y_pred_dt_train):.2f}, Train R²: {r2_score(Y_train, Y_pred_dt_train):.4f}")
print(f"Decision Tree - Test MSE: {mean_squared_error(Y_test, Y_pred_dt_test):.2f}, Test R²: {r2_score(Y_test, Y_pred_dt_test):.4f}")

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Initialize the Random Forest Regressor
rf = RandomForestRegressor(
    n_estimators=300,        # number of trees
    max_depth=15,            # depth of trees
    min_samples_split=5,     # minimum samples to split a node
    random_state=42
)

# Train the model
rf.fit(X_train, Y_train)

# Make predictions
Y_pred_train = rf.predict(X_train)
Y_pred_test = rf.predict(X_test)

# Evaluate performance
print(f"Random Forest - Train MSE: {mean_squared_error(Y_train, Y_pred_train):.2f}, R²: {r2_score(Y_train, Y_pred_train):.4f}")
print(f"Random Forest - Test MSE: {mean_squared_error(Y_test, Y_pred_test):.2f}, R²: {r2_score(Y_test, Y_pred_test):.4f}")

import xgboost as xgb
# Initialize and train XGBoost Regressor
xgb_reg = xgb.XGBRegressor(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

xgb_reg.fit(X_train, Y_train)

# Predictions
Y_pred_train = xgb_reg.predict(X_train)
Y_pred_test = xgb_reg.predict(X_test)

# Evaluation
print(f"XGBoost - Train MSE: {mean_squared_error(Y_train, Y_pred_train):.2f}, R²: {r2_score(Y_train, Y_pred_train):.4f}")
print(f"XGBoost - Test MSE: {mean_squared_error(Y_test, Y_pred_test):.2f}, R²: {r2_score(Y_test, Y_pred_test):.4f}")

import pickle

# Save the trained XGBoost model to a pickle file
with open('xgb_reg_model.pkl', 'wb') as file:
    pickle.dump(xgb_reg, file)

print("XGBoost model saved as pickle file.")

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

gbr = GradientBoostingRegressor(n_estimators=300, learning_rate=0.1, max_depth=5, random_state=42)
gbr.fit(X_train, Y_train)

Y_pred_train = gbr.predict(X_train)
Y_pred_test = gbr.predict(X_test)

print(f"Gradient Boosting - Train MSE: {mean_squared_error(Y_train, Y_pred_train):.2f}, R²: {r2_score(Y_train, Y_pred_train):.4f}")
print(f"Gradient Boosting - Test MSE: {mean_squared_error(Y_test, Y_pred_test):.2f}, R²: {r2_score(Y_test, Y_pred_test):.4f}")

from sklearn.model_selection import GridSearchCV
import xgboost as xgb
from sklearn.metrics import make_scorer, mean_squared_error

# Define the model
xgb_reg = xgb.XGBRegressor(random_state=42)

# Define the parameter grid to search
param_grid = {
    'n_estimators': [100, 300, 500],            # number of trees
    'max_depth': [3, 5, 6, 8],                  # depth of each tree
    'learning_rate': [0.01, 0.05, 0.1],         # step size shrinkage
    'subsample': [0.6, 0.8, 1.0],                # fraction of samples used per tree
    'colsample_bytree': [0.6, 0.8, 1.0],        # fraction of features used per tree
    'reg_alpha': [0, 0.1, 1],                    # L1 regularization
    'reg_lambda': [1, 1.5, 2],                   # L2 regularization
}

# Use negative MSE as scoring since GridSearchCV tries to maximize the score
neg_mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)

# Setup GridSearchCV
grid_search = GridSearchCV(
    estimator=xgb_reg,
    param_grid=param_grid,
    scoring=neg_mse_scorer,
    cv=3,
    verbose=1,
    n_jobs=-1
)

# Run the grid search
grid_search.fit(X_train, Y_train)

# Best hyperparameters
print("Best hyperparameters found:")
print(grid_search.best_params_)

# Use the best estimator to predict and evaluate
best_xgb = grid_search.best_estimator_

Y_pred_train = best_xgb.predict(X_train)
Y_pred_test = best_xgb.predict(X_test)

print(f"Tuned XGBoost - Train MSE: {mean_squared_error(Y_train, Y_pred_train):.2f}, R²: {r2_score(Y_train, Y_pred_train):.4f}")
print(f"Tuned XGBoost - Test MSE: {mean_squared_error(Y_test, Y_pred_test):.2f}, R²: {r2_score(Y_test, Y_pred_test):.4f}")

